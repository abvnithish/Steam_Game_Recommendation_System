{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghavjajodia/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import implicit\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy.core.umath_tests import inner1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghavjajodia/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "full_data = pd.read_csv('data/final_data.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cdfTransformer(object):\n",
    "    \n",
    "    transformerSeries = {}\n",
    "    \n",
    "    def __init__(self, useridcolname, itemidcolname, ratingcolname):\n",
    "        self.userid_colname = useridcolname\n",
    "        self.itemid_colname = itemidcolname\n",
    "        self.rating_colname = ratingcolname\n",
    "    \n",
    "    def getLowerOrEqualIndex(self,playtimeList, playtime):\n",
    "        if playtime < playtimeList[0]:\n",
    "            return 0\n",
    "        ans = 0\n",
    "        low = 0\n",
    "        high = len(playtimeList) - 1\n",
    "        while(low<=high):\n",
    "            mid = low + (high-low)//2\n",
    "            if playtime > playtimeList[mid]:\n",
    "                ans = mid\n",
    "                low = mid + 1\n",
    "            elif playtime == playtimeList[mid]:\n",
    "                return mid\n",
    "            else:\n",
    "                high = mid - 1\n",
    "        return ans      \n",
    "    \n",
    "    def getNearestCdf(self,appid, playtime):\n",
    "        playtimeList = self.transformerSeries[appid].index\n",
    "        bestpos = self.getLowerOrEqualIndex(playtimeList, playtime)\n",
    "        return self.transformerSeries[appid].iloc[bestpos]\n",
    "    \n",
    "    def fitTransform(self,tupledata):\n",
    "        grouped1 = tupledata.groupby([self.itemid_colname,self.rating_colname]).count()\n",
    "        grouped2 = grouped1.groupby(level=[0]).cumsum()\n",
    "        grouped3 = grouped2.groupby(level = [0]).max()\n",
    "        withcdf = grouped2/grouped3\n",
    "        self.transformerSeries = pd.Series(withcdf[self.userid_colname],index=withcdf.index)\n",
    "        withcdf_df = withcdf.reset_index(level=[0,1])\n",
    "        withcdf_df.rename(columns={self.userid_colname:'temp_rating'}, inplace=True)\n",
    "        finaltuple = pd.merge(withcdf_df,tupledata, on=[self.itemid_colname,self.rating_colname],how='inner',suffixes=('_newdf',''))\n",
    "        finaltuple.drop(self.rating_colname, inplace=True, axis = 1)\n",
    "        finaltuple.rename(columns={'temp_rating':self.rating_colname}, inplace=True)\n",
    "        return finaltuple\n",
    "\n",
    "    def Transform(self,tupledata):\n",
    "        ansdata = tupledata.groupby([self.itemid_colname,self.rating_colname]).count().reset_index()\n",
    "        ansdata.drop(self.userid_colname, inplace = True, axis = 1)\n",
    "        ansdata['rating_temp'] =  ansdata.apply(lambda x: self.getNearestCdf(x[self.itemid_colname],x[self.rating_colname]), axis = 1)\n",
    "        ansdata = pd.merge(ansdata,tupledata, on=[self.itemid_colname,self.rating_colname],how='inner',suffixes=('_newdf',''))\n",
    "        ansdata.drop(self.rating_colname, axis = 1, inplace = True)\n",
    "        ansdata.rename(columns = {\"rating_temp\":self.rating_colname}, inplace = True)         \n",
    "        return ansdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dff, split_ratio = 0.8):\n",
    "    \n",
    "    tr_sample = dff.groupby('sid').apply(lambda x:x.sample(frac = 0.8))\n",
    "    tr_sample_index = list(zip(*tr_sample.index))[1]\n",
    "    te_sample = dff[(dff.index.isin(tr_sample_index) == False)]\n",
    "    \n",
    "    tr_sample.index.rename(['id', 'appid_level'], inplace=True)\n",
    "    tr_sample.reset_index(drop = True, inplace = True)\n",
    "    te_sample.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return tr_sample, te_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matrixFactorizer(object):\n",
    "    \n",
    "    def __init__(self, latentFactors = 3, max_iterations = 20, reg = 0.01):        \n",
    "        self.numLatent = latentFactors\n",
    "        self.max_iterations = max_iterations\n",
    "        self.reg = reg\n",
    "    \n",
    "    def fit(self,train_ratingsbyuser, train_ratingsbyitem, total_users, total_items):\n",
    "        self.U = np.random.randn(total_users, self.numLatent) / self.numLatent\n",
    "        self.V = np.random.randn(self.numLatent, total_items) / self.numLatent\n",
    "        \n",
    "        for t in range(self.max_iterations):\n",
    "          # update U\n",
    "          for i in range(total_users):\n",
    "            if i in train_ratingsbyuser:\n",
    "              matrix = np.zeros((self.numLatent, self.numLatent)) + self.reg*np.eye(self.numLatent)\n",
    "              vector = np.zeros(self.numLatent)\n",
    "              for j, r in train_ratingsbyuser[i]:\n",
    "                matrix += np.outer(self.V[:,j], self.V[:,j])\n",
    "                vector += (r)*self.V[:,j]\n",
    "              self.U[i,:] = np.linalg.solve(matrix, vector)\n",
    "\n",
    "          # update V\n",
    "          for j in range(total_items):\n",
    "            if j in train_ratingsbyitem:\n",
    "              matrix = np.zeros((self.numLatent, self.numLatent)) + self.reg*np.eye(self.numLatent)\n",
    "              vector = np.zeros(self.numLatent)\n",
    "              for i, r in train_ratingsbyitem[j]:\n",
    "                matrix += np.outer(self.U[i,:], self.U[i,:])\n",
    "                vector += (r)*self.U[i,:]\n",
    "              self.V[:,j] = np.linalg.solve(matrix, vector)\n",
    "    \n",
    "    def predict(self, user_u, item_i):\n",
    "        return inner1d(self.U[user_u,:],np.transpose(self.V)[item_i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructCodes(df) :\n",
    "    \"\"\"\n",
    "    Maps 'steamid' to categorical code 'sid'\n",
    "    Maps 'appid' to categorical code 'aid'\n",
    "    Returns :\n",
    "        1. Dataframe with columns 'sid', 'aid', 'playtime_forever'\n",
    "        2. Dictionary mapping 'steamid' to 'sid'\n",
    "        3. Dictionary mapping 'sid' to 'steamid'\n",
    "        4. Dictionary mapping 'appid' to 'aid'\n",
    "        5. Dictionary mapping 'aid' to 'appid'\n",
    "    \"\"\"\n",
    "    coded_df = df.copy(deep = True)\n",
    "    coded_df[\"steamid\"] = coded_df[\"steamid\"].astype(\"category\")\n",
    "    coded_df[\"appid\"] = coded_df[\"appid\"].astype(\"category\")\n",
    "    coded_df[\"sid\"] = coded_df[\"steamid\"].cat.codes\n",
    "    coded_df[\"aid\"] = coded_df[\"appid\"].cat.codes\n",
    "    \n",
    "    sid_to_steamid = dict(enumerate(coded_df[\"steamid\"].cat.categories))\n",
    "    aid_to_appid = dict(enumerate(coded_df[\"appid\"].cat.categories))\n",
    "    steamid_to_sid = {v : k for k, v in sid_to_steamid.items()}\n",
    "    appid_to_aid = {v : k for k, v in aid_to_appid.items()}\n",
    "    \n",
    "    coded_df.drop([\"steamid\", \"appid\"], axis = 1, inplace = True)\n",
    "    \n",
    "    return(coded_df, steamid_to_sid, sid_to_steamid, appid_to_aid, aid_to_appid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructSparseMatrices(df) :\n",
    "    \"\"\"\n",
    "    Constructs sparse matrices that will be used in ALS optimization\n",
    "    Input : Dataframe with columns - 'sid', 'aid', 'playtime_forever'\n",
    "    \n",
    "    Returns :\n",
    "        1. User x Item Sparse Matrix\n",
    "        2. Item x User Sparse Matrix\n",
    "    \"\"\"\n",
    "    data_useritem = sparse.csr_matrix((df[\"playtime_forever\"], (df[\"sid\"], df[\"aid\"])))\n",
    "    data_itemuser = sparse.csr_matrix((df[\"playtime_forever\"], (df[\"aid\"], df[\"sid\"])))\n",
    "    \n",
    "    sid_unique = df[\"sid\"].nunique()\n",
    "    aid_unique = df[\"aid\"].nunique()\n",
    "    \n",
    "    assert data_useritem.shape == (sid_unique, aid_unique)\n",
    "    assert data_itemuser.shape == (aid_unique, sid_unique)\n",
    "    \n",
    "    return(data_useritem, data_itemuser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(data, factors, epochs, conf_func, alpha, lmbda) :\n",
    "    \"\"\"\n",
    "    Builds and trains Implicit Matrix Factorization model.\n",
    "    Input :\n",
    "        1. data - Item x User Sparse Matrix\n",
    "        2. factors - Number of latent factors\n",
    "        3. epochs - Number of iterations of ALS over the training data\n",
    "        4. conf_func - Confidence function\n",
    "        5. alpha - Confidence parameter\n",
    "        6. lmbda - Regularization parameter\n",
    "        \n",
    "    Output : model\n",
    "    \"\"\"\n",
    "    model = implicit.als.AlternatingLeastSquares(factors = factors, regularization = lmbda, iterations = epochs)\n",
    "    if conf_func == \"linear\" :\n",
    "        model.fit(alpha * data)\n",
    "    else :\n",
    "        print(\"{} is not a valid choice for conf_func. Choose one of the following : 'linear'\".format(conf_func))\n",
    "        return(None)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateValidationLoss(item_factors, user_factors, testtuples, itemidcolname, useridcolname, ratingcolname):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        1. item_factors - items * latentsize\n",
    "        2. user_factors - users * latentsize\n",
    "    Output : validationloss\n",
    "    \"\"\"\n",
    "    predictions = inner1d(item_factors[testtuples[itemidcolname],:],user_factors[testtuples[useridcolname],:])\n",
    "    return np.sqrt((((testtuples[ratingcolname] - predictions)**2).dot(np.ones(testtuples.shape[0])))/testtuples.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF():\n",
    "\n",
    "    def __init__(self, K, alpha, beta, iterations, ratingsbyuser, ratingsbyitem, traintuples, ratingcolname, itemcolname, usercolname):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty\n",
    "        entries in a matrix.\n",
    "\n",
    "        Arguments\n",
    "        - K (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        - ratings by user\n",
    "        - ratigns by item\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_users, self.num_items = len(ratingsbyuser.keys()), len(ratingsbyitem.keys()) \n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.ratingsbyuser = ratingsbyuser\n",
    "        self.ratingsbyitem = ratingsbyitem\n",
    "        self.traintuples = traintuples\n",
    "        self.ratingcolname = ratingcolname\n",
    "        self.itemcolname = itemcolname\n",
    "        self.usercolname = usercolname\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "\n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.traintuples[self.ratingcolname])\n",
    "\n",
    "        # Create a list of training samples\n",
    "        #self.samples = [\n",
    "        #    (i, j, self.R[i, j])\n",
    "        #    for i in range(self.num_users)\n",
    "        #    for j in range(self.num_items)\n",
    "        #    if self.R[i, j] > 0\n",
    "        #]\n",
    "\n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            self.traintuples.sample(frac=1).reset_index(drop=True)\n",
    "            self.sgd()\n",
    "            mse = self.mse()\n",
    "            training_process.append((i, mse))\n",
    "            print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "\n",
    "        return training_process\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        predictions_dot = inner1d(self.Q[self.traintuples[self.itemcolname],:],self.P[self.traintuples[self.usercolname],:])\n",
    "        biases = self.b_i[self.traintuples[self.itemcolname]] + self.b_u[self.traintuples[self.usercolname]] + self.b\n",
    "        predictions = predictions_dot + biases\n",
    "        return np.sqrt((((self.traintuples[self.ratingcolname] - predictions)**2).dot(np.ones(self.traintuples.shape[0])))/self.traintuples.shape[0])        \n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for index, row in self.traintuples.iterrows():\n",
    "            i,j,r = int(row['sid']),int(row['aid']),row['playtime_forever']\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "\n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "\n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "\n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2643226, 3), (660690, 3), (2643226, 3))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create coded data, and split in train,test\n",
    "coded_data, steamidtosid, sidtosteamid, appidtoaid, aidtoappid = constructCodes(full_data)\n",
    "train,test = train_test_split(coded_data)\n",
    "transformer1 = cdfTransformer('sid','aid','playtime_forever')\n",
    "traintuples = transformer1.fitTransform(train)\n",
    "train.shape, test.shape, traintuples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aid</th>\n",
       "      <th>playtime_forever</th>\n",
       "      <th>sid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.097124</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.097124</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.097124</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.097124</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.097124</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aid  playtime_forever  sid\n",
       "0    0          0.097124   30\n",
       "1    0          0.097124   73\n",
       "2    0          0.097124   75\n",
       "3    0          0.097124   76\n",
       "4    0          0.097124   83"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtuples = transformer1.Transform(test)\n",
    "testtuples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((660690, 3), 12982, 13845, 12858, 13845)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtuples.shape, traintuples['aid'].nunique(), traintuples['sid'].nunique(), testtuples['aid'].nunique(), testtuples['sid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12982"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data['appid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_mat, item_user_mat = constructSparseMatrices(traintuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13845, 12982)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n",
      "100%|██████████| 10.0/10 [00:02<00:00,  4.34it/s]\n",
      "100%|██████████| 10.0/10 [00:02<00:00,  4.10it/s]\n",
      "100%|██████████| 10.0/10 [00:02<00:00,  3.86it/s]\n",
      "100%|██████████| 10.0/10 [00:02<00:00,  3.66it/s]\n",
      "100%|██████████| 50.0/50 [00:15<00:00,  3.02it/s]\n",
      "100%|██████████| 50.0/50 [00:16<00:00,  3.09it/s]\n",
      "100%|██████████| 50.0/50 [00:16<00:00,  3.06it/s]\n",
      "100%|██████████| 50.0/50 [00:16<00:00,  3.05it/s]\n",
      "100%|██████████| 100.0/100 [00:33<00:00,  3.04it/s]\n",
      "100%|██████████| 100.0/100 [00:33<00:00,  2.94it/s]\n",
      "100%|██████████| 100.0/100 [00:34<00:00,  2.93it/s]\n",
      "100%|██████████| 100.0/100 [00:34<00:00,  2.93it/s]\n",
      "100%|██████████| 10.0/10 [00:03<00:00,  2.70it/s]\n",
      "100%|██████████| 10.0/10 [00:03<00:00,  2.74it/s]\n",
      "100%|██████████| 10.0/10 [00:03<00:00,  2.83it/s]\n",
      "100%|██████████| 10.0/10 [00:03<00:00,  2.58it/s]\n",
      "100%|██████████| 50.0/50 [00:18<00:00,  2.76it/s]\n",
      "100%|██████████| 50.0/50 [00:18<00:00,  2.84it/s]\n",
      "100%|██████████| 50.0/50 [00:18<00:00,  2.74it/s]\n",
      "100%|██████████| 50.0/50 [00:18<00:00,  2.73it/s]\n",
      "100%|██████████| 100.0/100 [00:37<00:00,  2.77it/s]\n",
      "100%|██████████| 100.0/100 [00:36<00:00,  2.75it/s]\n",
      "100%|██████████| 100.0/100 [00:36<00:00,  2.81it/s]\n",
      "100%|██████████| 100.0/100 [00:37<00:00,  2.75it/s]\n",
      "100%|██████████| 10.0/10 [00:04<00:00,  2.20it/s]\n",
      "100%|██████████| 10.0/10 [00:03<00:00,  2.28it/s]\n",
      "100%|██████████| 10.0/10 [00:04<00:00,  2.00it/s]\n",
      "100%|██████████| 10.0/10 [00:04<00:00,  2.18it/s]\n",
      "100%|██████████| 50.0/50 [00:22<00:00,  2.26it/s]\n",
      "100%|██████████| 50.0/50 [00:22<00:00,  2.19it/s]\n",
      "100%|██████████| 50.0/50 [00:22<00:00,  2.23it/s]\n",
      "100%|██████████| 50.0/50 [00:22<00:00,  2.26it/s]\n",
      "100%|██████████| 100.0/100 [00:44<00:00,  2.21it/s]\n",
      "100%|██████████| 100.0/100 [00:44<00:00,  2.24it/s]\n",
      "100%|██████████| 100.0/100 [00:44<00:00,  2.22it/s]\n",
      "100%|██████████| 100.0/100 [00:45<00:00,  2.23it/s]\n",
      "100%|██████████| 10.0/10 [00:05<00:00,  1.84it/s]\n",
      "100%|██████████| 10.0/10 [00:03<00:00,  2.54it/s]\n",
      "100%|██████████| 10.0/10 [00:04<00:00,  1.90it/s]\n",
      "100%|██████████| 10.0/10 [00:05<00:00,  1.86it/s]\n",
      "100%|██████████| 50.0/50 [00:26<00:00,  1.84it/s]\n",
      "100%|██████████| 50.0/50 [00:26<00:00,  1.93it/s]\n",
      "100%|██████████| 50.0/50 [00:27<00:00,  1.83it/s]\n",
      "100%|██████████| 50.0/50 [00:26<00:00,  1.95it/s]\n",
      "100%|██████████| 100.0/100 [00:53<00:00,  1.88it/s]\n",
      "100%|██████████| 100.0/100 [00:53<00:00,  1.89it/s]\n",
      "100%|██████████| 100.0/100 [00:53<00:00,  1.87it/s]\n",
      "100%|██████████| 100.0/100 [00:53<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "factors = [5,10,50,100]\n",
    "epochs = [10,50,100]\n",
    "lambdas = [0.001,0.01,0.1,1]\n",
    "\n",
    "allModels = {}\n",
    "bestLoss = 10000000\n",
    "\n",
    "for factor in factors:\n",
    "    for epoch in epochs:\n",
    "        for lamb in lambdas:\n",
    "            tempmodel = trainModel(item_user_mat,factor, epoch,'linear',1,lamb)\n",
    "            valLoss = evaluateValidationLoss(tempmodel.item_factors, tempmodel.user_factors, testtuples, 'aid', 'sid', 'playtime_forever')\n",
    "            key = str(factor) + \"_\" + str(epoch) + \"_\" + str(lamb)\n",
    "            allModels[key] = tempmodel\n",
    "            \n",
    "            if(valLoss < bestLoss):\n",
    "                bestLoss = valLoss\n",
    "                bestConfiguration = (factor, epoch, lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('implicitresults2.txt','w')\n",
    "for k in allModels:\n",
    "    mod = allModels[k]\n",
    "    valLoss = evaluateValidationLoss(mod.item_factors, mod.user_factors, testtuples, 'aid', 'sid', 'playtime_forever')\n",
    "    f.write(str(k) + \"   \" + str(valLoss) + '\\n')\n",
    "f.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('implicitresults.txt','a')\n",
    "f.write(str(allModels))\n",
    "f.cstrlose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n",
      "100%|██████████| 10.0/10 [00:02<00:00,  4.17it/s]\n"
     ]
    }
   ],
   "source": [
    "model = trainModel(item_user_mat, 10, 10, 'linear', 1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 10, 0.1)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4449343065072283"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateValidationLoss(model.item_factors, model.user_factors, testtuples, 'aid', 'sid','playtime_forever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "temptraintuples = traintuples.copy(deep=True)\n",
    "temptraintuples['tup'] = list(zip(temptraintuples['sid'], temptraintuples['playtime_forever']))\n",
    "ratingsbyitem = temptraintuples.groupby('aid')['tup'].apply(list).to_dict()\n",
    "\n",
    "temptraintuples2 = traintuples.copy(deep=True)\n",
    "temptraintuples2['tup'] = list(zip(temptraintuples2['aid'], temptraintuples2['playtime_forever']))\n",
    "ratingsbyuser = temptraintuples2.groupby('sid')['tup'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "matFact1 = matrixFactorizer(latentFactors=20)\n",
    "matFact1.fit(ratingsbyuser, ratingsbyitem, traintuples['sid'].nunique(), traintuples['aid'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.286446025915094"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateValidationLoss(np.transpose(matFact1.V), matFact1.U, testtuples, 'aid', 'sid', 'playtime_forever')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
